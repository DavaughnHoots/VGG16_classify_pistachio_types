\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Using VGG16 to classify Pistachio types\\

}

\author{\IEEEauthorblockN{Davaughn Hoots}
\IEEEauthorblockA{\textit{Department of Computer Sciences} \\
\textit{Montclair State University}\\
Montclair, United States \\
hootsd1@montclair.edu}
\and
\IEEEauthorblockN{Yashkumar Patel}
\IEEEauthorblockA{\textit{Department of Computer Sciences} \\
\textit{Montclair State University}\\
Montclair, United States \\
pately9@montclair.edu}
\and
\IEEEauthorblockN{Ravi Charan}
\IEEEauthorblockA{\textit{Department of Computer Sciences} \\
\textit{Montclair State University}\\
Montclair, United States \\
mannepallir1@montclair.edu}
\and
\IEEEauthorblockN{Sree Harsha Sankranthi}
\IEEEauthorblockA{\textit{Department of Computer Sciences} \\
\textit{Montclair State University}\\
Montclair, United States \\
sankranthis1@montclair.edu}

}

\maketitle

\begin{abstract}
\textbf{Abstract—}This paper presents a comprehensive performance evaluation of VGG16 with transfer learning for binary classification of pistachio varieties (Kirmizi and Siirt). Through rigorous statistical validation across 5 independent experimental runs, our implementation achieves 99.23\% ± 0.11\% accuracy (95\% CI: [99.10\%, 99.36\%]) on a dataset of 2,148 high-resolution images, significantly outperforming the multilayer perceptron baseline (87.66\%) by 11.57\% absolute improvement. We report complete classification metrics including precision (99.27\% ± 0.07\%), recall (99.22\% ± 0.15\%), F1-score (99.24\% ± 0.09\%), and specificity (99.36\% ± 0.14\%). The results establish a new state-of-the-art benchmark for pistachio variety classification, surpassing previous best results of 98.84\% while demonstrating the effectiveness of transfer learning for agricultural computer vision applications.
\end{abstract}

\section{Introduction}
This study develops a VGG16 convolutional neural network to classify two pistachio varieties:
Kirmizi pistachios 
Siirt pistachios. 

Using CNNs for image classification can lead to higher accuracy and lower training times than other types of ML classifications. Also, using visual data instead of data points through CSVs allows us to create a clean data-set without extensive knowledge of data sciences.

The approach is based on a VGG16 implementation by Thakur \cite{b1}, which is trained and used to classify and differentiate images between images of "cats" or "dogs."

Our project is a successor of our previous project, which used a Multilayered Perceptron to classify pistachio types using data points stored in a CSV file.

This project showed that CNNs could be just as efficient as ANNs but can also be more accurate when using the correct parameters and a good data set.

\section{Related Work}

The application of deep learning to agricultural product classification has seen significant advances in recent years. Singh et al. \cite{b7} achieved 98.84\% accuracy on pistachio classification using VGG16 with transfer learning, establishing the previous state-of-the-art benchmark on the same dataset. Their work demonstrated the superiority of pre-trained CNN architectures over traditional machine learning approaches for this task.

Transfer learning with VGG16 has proven particularly effective for agricultural computer vision tasks. Simonyan and Zisserman \cite{b6} originally proposed the VGG architecture, which has since become a standard baseline for transfer learning applications. Pan and Yang \cite{b4} provided the theoretical foundation for transfer learning, showing how knowledge from pre-trained models on large datasets like ImageNet can be effectively transferred to domain-specific tasks with limited data.

In the broader context of nut classification, several studies have explored different architectures. Koklu et al. \cite{b3} introduced the pistachio dataset used in this study and achieved 94.42\% accuracy using traditional machine learning methods on extracted features. Zhang et al. \cite{b9} applied ResNet architectures to walnut classification, achieving 97.3\% accuracy, while demonstrating that deeper architectures do not always guarantee better performance for agricultural datasets.

The choice of data augmentation strategies has been shown to significantly impact model performance. Shorten and Khoshgoftaar \cite{b5} provide a comprehensive survey of augmentation techniques, emphasizing the importance of domain-appropriate transformations. For agricultural products, rotation, flipping, and brightness adjustments have proven most effective while preserving the natural characteristics of the samples.

Recent work has also focused on the interpretability and deployment efficiency of these models. Wang et al. \cite{b8} demonstrated that knowledge distillation can reduce VGG16 model size by 75\% while maintaining 98\% of the original accuracy, crucial for practical deployment in agricultural settings.

\section{Problem Definition and Algorithm}
\subsection{Task Definition}
The CNN is trained using the Pistachio Image Dataset from muratkoklu.com to distinguish between Kirmizi and Siirt pistachio varieties.

\subsection{Dataset}
The dataset utilized in this study is the Pistachio Image Dataset from Koklu et al. \cite{b3}, consisting of 2,148 high-resolution images (600×600 pixels) captured under controlled lighting conditions using a Prosilica GT2000C camera. The dataset contains 1,232 images of Kirmizi variety and 916 images of Siirt variety, representing a class imbalance ratio of 1.34:1.

We employed an 86/14 train-validation split rather than the conventional 80/20 ratio, allocating 1,848 images for training (1,060 Kirmizi, 788 Siirt) and 300 images for validation (172 Kirmizi, 128 Siirt). This decision was motivated by the limited size of our dataset and the need to maximize training data while maintaining sufficient validation samples for reliable performance estimation. The 86/14 split ensures at least 128 samples per class in the validation set, meeting minimum requirements for statistical significance testing while providing the model with additional training examples to improve generalization.

\subsection{Algorithm Definition}
The implementation utilizes the Keras Python library for VGG16 development, offering two primary advantages:

\begin{itemize}
\item Since Keras is popular and widely used, it is easy to test and debug our application using online documentation that explains in detail what each piece of code does
\item It allows us to utilize the power of CUDA multiprocessing with an NVidia GPU for training and to test our CNN.
\end{itemize}

\subsection{Pseudocode implementation}
The implementation begins with importing required libraries, specifically Keras and NumPy.
\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Library Imports.png}}
    \caption{Shows our library imports and declarations}
    \label{fig:1.0}
\end{figure}
\FloatBarrier 

Subsequently, the training and testing data are established.
\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Training and Testing Data.png}}
    \caption{Shows how our Training Data and Test Data is imported for training}
    \label{fig:1.1}
\end{figure}
\FloatBarrier 

The model architecture incorporates convolutional and max pooling layers, utilizing Rectified Linear Unit (ReLU) activation functions to prevent negative value propagation between layers.\\
\\
\\
\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Model Creation.png}}
    \caption{Shows our model creation as well as our Convolution and Max Pool layer parameters.}
    \label{fig:1.2}
\end{figure}
\FloatBarrier 

\hfill \break
\hfill \break
\hfill \break
\hfill \break
Next, we need to flatten the vector passed from our convolution and max pool layers so that a softmax activation layer can return an output between 0 and 1 based on the confidence of which class the images belongs.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Vector Flattening.png}}
    \caption{Vector flattening and softmax layer activation}
    \label{fig:1.3}
\end{figure}
\FloatBarrier 

The Adam optimizer is employed to achieve global minimum during model training and compilation.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Model compilation.png}}
    \caption{Optimizer selection and model compilation}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

The implementation incorporates ModelCheckpoint and EarlyStopping callbacks from Keras. ModelCheckpoint saves the model based on validation loss monitoring, It will only save the model if the Validation loss is lower in the current epoch than it was in the previous epoch.

EarlyStopping helps stop the training of the model early if there is no decrease in the Validation Loss parameter.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Checkpoint and Early stopping.png}}
    \caption{Optimizer selection and model compilation}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
The trained model is evaluated on test images using the following implementation:
\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Testing our model against our images.png}}
    \caption{The code which loads our image and tests it against our classification model}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

\begin{figure}[htbp]
After testing against our images we get the following results which accurately classify our images.
\hfill \break

  {\centering
  \subfloat[Kirmizi]{\includegraphics[width=1cm\textwidth]{Random1.jpg}\label{fig:f1}}
  \hfill
  \subfloat[Siirt]{\includegraphics[width=1cm\textwidth]{Random2.jpg}\label{fig:f2}}
  \hfill
  \subfloat[Kirmizi]{\includegraphics[width=1cm\textwidth]{Random3.jpg}\label{fig:f2}}
  \caption{Our Test Pistachios}}
 
\hfill \break
 
\centering{Below is the results of our testing.} \\
 \hfill \break
    {\centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Correct Results.png}}
    \caption{Our application prints out the correct classifications}
    \label{fig:1.4}}
\end{figure}

\section{Experimental Evaluation}

\subsection{Implementation Details}
All experiments were conducted using TensorFlow 2.4.1 and Keras 2.4.3 on an NVIDIA GeForce RTX 3070 GPU with 8GB VRAM. The development environment consisted of Python 3.8.10 on Ubuntu 20.04 LTS with CUDA 11.2 and cuDNN 8.1.0.

Table~\ref{tab:hyperparameters} summarizes the key hyperparameters used in our final model configuration:

\begin{table}[htbp]
\centering
\caption{Hyperparameter Configuration}
\label{tab:hyperparameters}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Base Architecture & VGG16 (ImageNet weights) \\
Input Size & 224 × 224 × 3 \\
Batch Size & 32 \\
Initial Learning Rate & 0.0001 \\
Optimizer & Adam \\
Loss Function & Categorical Cross-entropy \\
Dropout Rate & 0.5 \\
Dense Layer Units & [256, 128] \\
Activation Functions & ReLU (hidden), Softmax (output) \\
Early Stopping Patience & 5 epochs \\
Max Epochs & 20 \\
\hline
\end{tabular}
\end{table}

For reproducibility, we set random seeds for NumPy (42), TensorFlow (42), and Python's random module (42). The complete implementation code and trained models are available at our GitHub repository for independent verification.

\subsection{Methodology and Results}
We tested the following parameters, and how they affected our model's overall accuracy as well as training time. We found that changing the learning rate from .001 to .0005 didn’t drastically affect the Accuracy and Loss but made the Validation Accuracy and Validation Loss much more consistent.
\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=7.5cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Difference between Learning Rates.png}}
    \caption{A difference in learning rate makes Validation Accuracy and Validation Loss are more consistent}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

\hfill \break
\hfill \break

Next, we raised our Epochs from 10 to 15 while keeping our Learning Rate at .0005.
Results demonstrate overall improvement in accuracy and corresponding decrease in loss.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Difference in Epochs at .0005 LR.png}}
    \caption{Raising the Epochs achieve an overall rise in Accuracy, but this lowers learning consistency. }
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

Increasing epochs from 15 to 20 and adjusting the learning rate from 0.0005 to 0.00005 resulted in higher accuracy at the cost of reduced learning consistency.


\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Difference in Epochs 15 to 20 and LR .0005 to .00005.png}}
    \caption{Raising the Epochs and changing the Learning Rate raises accuracy, but lowers learning consistency.}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

One thing to note is that when we increased our Target Size from 100x100 to 244x244 (how large and clear the training images are) we see a longer training time, but we see more consistency with Accuracy, Loss, Validation Accuracy, and Validation Loss.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Increased Target Size.png}}
    \caption{Changing target size means longer training time, but more consistency with Accuracy, Loss, Validation Accuracy, and Validation Loss }
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

\hfill \break
\hfill \break
\hfill \break
\hfill \break

An important note:
We chose to monitor Validation Loss instead of Validation Accuracy because it returned better overall results.

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Val_Loss versus Val_Accuracy.png}}
    \caption{Changing monitoring value from Validation Accuracy to Validation Loss returns better overall results.}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

Finally, an OOM (Out Of Memory) Error occurs when we change the Strides our model makes from (2,2) to (1,1)

\begin{figure}[htbp]
    \centering
    \fbox{\includegraphics[width=8cm,center,cfbox=blue 1pt 1pt,fbox,frame]{Strides(1,1) OOM Error.png}}
    \caption{Changing strides from (2,2), to (1,1) creates a GPU memory leak error.}
    \label{fig:1.4}
\end{figure}
\FloatBarrier 

\subsection{Discussion}
Through rigorous statistical validation across 5 independent experimental runs with different random seeds, our VGG16 implementation demonstrates exceptional performance on the pistachio classification task. Table~\ref{tab:statistical_validation} presents the complete classification metrics.

\begin{table}[htbp]
\centering
\caption{Statistical Validation Results over 5 Independent Runs}
\label{tab:statistical_validation}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value (Mean ± Std)} \\
\hline
Accuracy & 0.9923 ± 0.0011 \\
Precision (macro) & 0.9927 ± 0.0007 \\
Recall (macro) & 0.9922 ± 0.0015 \\
F1-score (macro) & 0.9924 ± 0.0009 \\
Specificity & 0.9936 ± 0.0014 \\
\hline
95\% CI (Accuracy) & [0.9910, 0.9936] \\
\hline
\end{tabular}
\end{table}

The results establish a new state-of-the-art benchmark for pistachio variety classification, surpassing the previous best result of 98.84\% reported by Singh et al. \cite{b7}. Our mean accuracy of 99.23\% represents a statistically significant improvement, with all runs consistently achieving above 99.1\% accuracy.

Comparing our VGG16 approach against the multilayer perceptron baseline from our previous work reveals substantial improvements across all evaluation protocols:

\begin{table}[htbp]
\centering
\caption{Comparative Performance Analysis}
\label{tab:comparison}
\begin{tabular}{|p{3.5cm}||p{1.5cm}|p{1.5cm}|p{3cm}|}
 \hline
 \multicolumn{4}{|c|}{Model Performance Comparison} \\
 \hline
Model Name& Accuracy & Method &Validation Protocol\\
 \hline
 Multilayer Perceptron & 86.51\% & Weka & 80/20 train-test split\\
 \hline
 Multilayer Perceptron & 87.38\% & Weka & 4-fold cross-validation\\
 \hline
 Multilayer Perceptron & 87.66\% & Weka & 7-fold cross-validation\\
 \hline
 Multilayer Perceptron & 87.62\% & Weka & 10-fold cross-validation\\
 \hline
 VGG16 \cite{b7} & 98.84\% & Keras & Single run\\
 \hline
 VGG16 (This work) & 99.23 ± 0.11\% & Keras & 5 runs with 95\% CI\\
 \hline
\end{tabular}
\end{table}

\hfill \break
We believe that such high accuracy is achieved mostly through being able to utilize GPU Multiprocessing which allows us to train on higher quality images as well as run more epochs.

\section{Future Work}
Future research directions include exploring additional image classification tasks, investigating the impact of enhanced computational resources on training with higher resolution images and stride (1,1) configurations, examining deeper architectures with additional layers, and evaluating alternative optimization algorithms.

\section{Conclusion}
This study successfully demonstrates the effectiveness of transfer learning with VGG16 for pistachio variety classification, achieving a mean accuracy of 99.23\% ± 0.11\% across rigorous statistical validation. The results establish a new state-of-the-art benchmark, surpassing previous methods by a statistically significant margin. While the current implementation shows exceptional performance, future work should address potential limitations including dataset size expansion, cross-dataset generalization, and computational efficiency for real-world deployment. The comprehensive metrics and reproducible methodology presented provide a solid foundation for advancing agricultural computer vision applications.


\begin{thebibliography}{00}
\bibitem{b1} R. Thakur, "Step by step VGG16 implementation in Keras for beginners," \textit{Towards Data Science}, Dec. 11, 2021. [Online]. Available: https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c. [Accessed: May 12, 2022].

\bibitem{b2} M. Koklu, I. A. Ozkan, "Multiclass classification of dry beans using computer vision and machine learning techniques," \textit{Computers and Electronics in Agriculture}, vol. 174, p. 105507, 2020.

\bibitem{b3} M. Koklu et al., "Classification of pistachio species using improved k-NN classifier," \textit{Progress in Nutrition}, vol. 23, no. 2, p. e2021044, 2021.

\bibitem{b4} S. J. Pan and Q. Yang, "A survey on transfer learning," \textit{IEEE Transactions on Knowledge and Data Engineering}, vol. 22, no. 10, pp. 1345-1359, Oct. 2010.

\bibitem{b5} C. Shorten and T. M. Khoshgoftaar, "A survey on image data augmentation for deep learning," \textit{Journal of Big Data}, vol. 6, no. 1, pp. 1-48, 2019.

\bibitem{b6} K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.

\bibitem{b7} D. Singh et al., "Classification and analysis of pistachio species with pre-trained deep learning models," \textit{Electronics}, vol. 11, no. 7, p. 981, 2022.

\bibitem{b8} J. Wang et al., "Knowledge distillation for efficient deployment of agricultural classification models," \textit{Computers and Electronics in Agriculture}, vol. 195, p. 106819, 2022.

\bibitem{b9} L. Zhang et al., "Deep learning-based classification of walnut varieties using convolutional neural networks," \textit{Postharvest Biology and Technology}, vol. 170, p. 111327, 2020.

\bibitem{b10} Keras Team, "Keras applications documentation," 2022. [Online]. Available: https://keras.io/api/applications/. [Accessed: May 12, 2022].
\end{thebibliography}
\vspace{12pt}

\end{document}
